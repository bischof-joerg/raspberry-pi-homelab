# docker-compose.yml
# Monitoring stack for Raspberry Pi (ARM64) with IaC + basic hardening
# Commit A: Add VictoriaMetrics components alongside Prometheus (safe cutover).
# - Exposes ONLY Grafana by default
# - Prometheus/Alertmanager/VictoriaMetrics UIs are kept localhost-bound (enable LAN only if you really need it)

name: ${COMPOSE_PROJECT_NAME:-homelab-home-prod-mon}

services:
  prometheus:
    image: prom/prometheus:v3.1.0
    restart: unless-stopped
    networks: [monitoring]
    # Expose Prometheus UI only if you need it (prefer reverse proxy + auth/IP restriction)
    # ports: # for checking only
    #  - "9090:9090"
    ports:
      - "127.0.0.1:9090:9090"
    volumes:
      - /srv/data/stacks/monitoring/prometheus:/prometheus
      - ../prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ../prometheus/rules:/etc/prometheus/rules:ro
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.retention.time=30d
      - --web.enable-lifecycle
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "wget", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 4G # Prevents OOM on Pi 5 during heavy queries
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    security_opt:
      - no-new-privileges:true
    cap_drop: [ALL]
    read_only: true
    tmpfs:
      - /tmp

  alertmanager:
    image: prom/alertmanager:v0.28.0
    restart: unless-stopped
    networks: [monitoring]
    ports:
      - "127.0.0.1:9093:9093"
    volumes:
      - /srv/data/stacks/monitoring/alertmanager:/alertmanager
      # Config + templates come from the single volume prepared by the renderer
      - /srv/data/stacks/monitoring/alertmanager-config:/etc/alertmanager:ro
    command:
      - --config.file=/etc/alertmanager/alertmanager.yml
      - --storage.path=/alertmanager
    depends_on:
      alertmanager-config-render:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "wget", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
    security_opt:
      - no-new-privileges:true
    cap_drop: [ALL]
    read_only: true
    tmpfs:
      - /tmp

  # One-shot renderer: renders alertmanager.yml from template using envsubst and copies templates into the same volume.
  alertmanager-config-render:
    image: alpine:3.20
    restart: "no"
    networks: [monitoring]

    # Compose interpolation must not hard-fail if SMTP isn't configured yet.
    # Strict validation happens at runtime only when ALERT_EMAIL_ENABLED=1.
    environment:
      ALERT_EMAIL_ENABLED: ${ALERT_EMAIL_ENABLED:-0}

      ALERT_EMAIL_TO: ${ALERT_EMAIL_TO:-}
      ALERT_SMTP_SMARTHOST: ${ALERT_SMTP_SMARTHOST:-}
      ALERT_SMTP_FROM: ${ALERT_SMTP_FROM:-}
      ALERT_SMTP_AUTH_USERNAME: ${ALERT_SMTP_AUTH_USERNAME:-}
      ALERT_SMTP_AUTH_PASSWORD: ${ALERT_SMTP_AUTH_PASSWORD:-}
      ALERT_SMTP_REQUIRE_TLS: ${ALERT_SMTP_REQUIRE_TLS:-}

    volumes:
      - /srv/data/stacks/monitoring/alertmanager-config:/out
      - ../alertmanager/alertmanager.yml.tmpl:/in/alertmanager.yml.tmpl:ro
      - ../alertmanager/templates:/in/templates:ro

    command:
      - /bin/sh
      - -lc
      - |
        set -euo pipefail
        apk add --no-cache gettext

        if [ "$${ALERT_EMAIL_ENABLED:-0}" = "1" ]; then
          # Hard fail early if required env is missing. Prevents rendering empty config fields.
          : "$${ALERT_EMAIL_TO:?missing ALERT_EMAIL_TO}"
          : "$${ALERT_SMTP_SMARTHOST:?missing ALERT_SMTP_SMARTHOST}"
          : "$${ALERT_SMTP_FROM:?missing ALERT_SMTP_FROM}"
          : "$${ALERT_SMTP_AUTH_USERNAME:?missing ALERT_SMTP_AUTH_USERNAME}"
          : "$${ALERT_SMTP_AUTH_PASSWORD:?missing ALERT_SMTP_AUTH_PASSWORD}"
          # ALERT_SMTP_REQUIRE_TLS=true|false.
          : "$${ALERT_SMTP_REQUIRE_TLS:?missing ALERT_SMTP_REQUIRE_TLS}"

          # These blocks are inserted under:
          #   - name: "email-critical"
          #   - name: "email-warning"
          # in alertmanager.yml.tmpl.
          #
          # IMPORTANT: The indentation here must match the template location.
          # We start at 4 spaces so it becomes valid YAML under the receiver item.
          ALERT_EMAIL_CRITICAL_YAML="$(printf '%s\n' \
            '    email_configs:' \
            '      - to: "${ALERT_EMAIL_TO}"' \
            '        from: "${ALERT_SMTP_FROM}"' \
            '        smarthost: "${ALERT_SMTP_SMARTHOST}"' \
            '        auth_username: "${ALERT_SMTP_AUTH_USERNAME}"' \
            '        auth_password: "${ALERT_SMTP_AUTH_PASSWORD}"' \
            '        require_tls: ${ALERT_SMTP_REQUIRE_TLS}' \
            '        send_resolved: true'
          )"
          ALERT_EMAIL_WARNING_YAML="$${ALERT_EMAIL_CRITICAL_YAML}"
        else
          # Email disabled: keep receivers defined, but empty (no-op receivers).
          ALERT_EMAIL_CRITICAL_YAML=""
          ALERT_EMAIL_WARNING_YAML=""
        fi

        export ALERT_EMAIL_CRITICAL_YAML ALERT_EMAIL_WARNING_YAML

        # Render config (limit substitution to our known variables)
        envsubst '$ALERT_EMAIL_CRITICAL_YAML $ALERT_EMAIL_WARNING_YAML \
          $ALERT_EMAIL_TO $ALERT_SMTP_SMARTHOST $ALERT_SMTP_FROM \
          $ALERT_SMTP_AUTH_USERNAME $ALERT_SMTP_AUTH_PASSWORD $ALERT_SMTP_REQUIRE_TLS' \
          < /in/alertmanager.yml.tmpl > /out/alertmanager.yml
        chmod 0644 /out/alertmanager.yml

        # Copy templates (optional; works even if empty)
        mkdir -p /out/templates
        if [ -d /in/templates ]; then
          cp -a /in/templates/. /out/templates/ 2>/dev/null || true
        fi
        chmod -R a+rX /out/templates || true

        echo "Rendered /out/alertmanager.yml and copied templates to /out/templates"
        test -s /out/alertmanager.yml

    security_opt:
      - no-new-privileges:true
    cap_drop: [ALL]
    # IMPORTANT: must be able to write to /out
    read_only: false
    # no mpfs: - /tmp section as the alertmanager-config-render service is a one-shot initialization container (it runs once and exits).
    # it woult only add slight overhead and is not needed.

  # --- New: VictoriaMetrics single-node TSDB ---
  victoriametrics:
    image: victoriametrics/victoria-metrics:v1.135.0
    restart: unless-stopped
    networks: [monitoring]
    # Localhost-bound UI/API for debugging; remove or move behind Traefik later.
    ports:
      - "127.0.0.1:8428:8428"
    volumes:
      - /srv/data/stacks/monitoring/victoriametrics:/victoria-metrics-data
    command:
      - -storageDataPath=/victoria-metrics-data
      - -retentionPeriod=30d
      - -httpListenAddr=:8428
    # temporarily disable healthchecks as it seems that endpoints are not available
    # healthcheck:
    #   test: ["CMD", "wget", "--tries=1", "--spider", "http://localhost:8428/health"]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 3
    deploy:
      resources:
        limits:
          memory: 4G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    security_opt:
      - no-new-privileges:true
    cap_drop: [ALL]
    read_only: true
    tmpfs:
      - /tmp

  # --- New: scrape agent -> remote_write into VictoriaMetrics ---
  vmagent:
    image: victoriametrics/vmagent:v1.135.0
    restart: unless-stopped
    networks: [monitoring]
    depends_on:
      victoriametrics:
        condition: service_started
    volumes:
      - ../vmagent/vmagent.yml:/etc/vmagent/vmagent.yml:ro
    command:
      - -promscrape.config=/etc/vmagent/vmagent.yml
      - -remoteWrite.url=http://victoriametrics:8428/api/v1/write
      - -httpListenAddr=:8429
      - -promscrape.suppressScrapeErrors
      - -remoteWrite.tmpDataPath=/tmp/vmagent-remotewrite-data
    healthcheck:
      test: ["CMD", "wget", "--tries=1", "--spider", "http://127.0.0.1:8429/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 20s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
    security_opt:
      - no-new-privileges:true
    cap_drop: [ALL]
    read_only: true
    tmpfs:
      - /tmp

  # --- New: alert rules evaluation -> Alertmanager ---
  vmalert:
    image: victoriametrics/vmalert:v1.135.0
    restart: unless-stopped
    networks: [monitoring]
    depends_on:
      victoriametrics:
        condition: service_started
      alertmanager:
        condition: service_started
    # Localhost-bound UI for debugging.
    ports:
      - "127.0.0.1:8880:8880"
    volumes:
      - ../vmalert/rules:/etc/vmalert/rules:ro
    command:
      - -rule=/etc/vmalert/rules/*.yml
      - -datasource.url=http://victoriametrics:8428
      - -notifier.url=http://alertmanager:9093
      - -remoteWrite.url=http://victoriametrics:8428
      - -remoteRead.url=http://victoriametrics:8428
      - -httpListenAddr=:8880
    healthcheck:
      test: ["CMD", "wget", "--tries=1", "--spider", "http://127.0.0.1:8880/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 20s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
    security_opt:
      - no-new-privileges:true
    cap_drop: [ALL]
    read_only: true
    tmpfs:
      - /tmp

  grafana:
    image: grafana/grafana:11.4.2
    restart: unless-stopped
    networks: [monitoring]
    ports:
      - "3000:3000"
    environment:
      # secrets in env_file (and keep it out of git)
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_AUTH_ANONYMOUS_ENABLED: "false"
      # Optional: enable Grafana's Prometheus metrics endpoint
      GF_METRICS_ENABLED: "true"
      # Disabling sending usage statistics to Grafana Labs
      GF_ANALYTICS_REPORTING_ENABLED: "false"
      GF_SERVER_ENABLE_GZIP: "true" # Better performance on Pi
    volumes:
      - /srv/data/stacks/monitoring/grafana:/var/lib/grafana
      - ../grafana/provisioning:/etc/grafana/provisioning:ro
      - ../grafana/dashboards:/var/lib/grafana/dashboards:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
    security_opt:
      - no-new-privileges:true
    cap_drop: [ALL]
    # Grafana needs write access to /var/lib/grafana (volume), so not fully read-only
    tmpfs:
      - /tmp

  node-exporter:
    image: quay.io/prometheus/node-exporter:v1.8.2
    restart: unless-stopped
    networks: [monitoring]
    # Do NOT expose to LAN; Prometheus scrapes it via Docker network
    # ports:
    #   - "9100:9100"
    pid: host
    user: "65534:65534"
    command:
      - --path.rootfs=/host
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|run|var/lib/docker/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|nsfs|overlay|proc|pstore|rpc_pipefs|securityfs|squashfs|sysfs|tracefs)$
    volumes:
      - /:/host:ro,rslave
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
    security_opt:
      - no-new-privileges:true
    cap_drop: [ALL]
    read_only: true
    tmpfs:
      - /tmp

  cadvisor:
    image: ghcr.io/google/cadvisor:0.56.2
    restart: unless-stopped
    # Use root and privileged to ensure cgroup v2 visibility on Pi 5 kernel
    user: root
    # This is non-negotiable on the Pi 5. cAdvisor must be able to move across namespaces to inspect other containers' resource usage.
    privileged: true
    networks: [monitoring]
    # Do NOT expose to LAN; Prometheus scrapes it via Docker network
    # ports:
    #   - "8080:8080"
    pid: host
    devices:
      - /dev/kmsg:/dev/kmsg
    volumes:
      - /etc/machine-id:/etc/machine-id:ro
      - /run/containerd/containerd.sock:/run/containerd/containerd.sock:ro
      - /sys/fs/cgroup:/sys/fs/cgroup:ro # Explicit cgroup v2 mount
      - /:/rootfs:ro
      - /var/run/docker.sock:/var/run/docker.sock:rw
      - /sys:/sys:ro
      - /var/lib/docker:/var/lib/docker:ro
    command: # recommended flags for Prometheus integration + performance on Pi
      # Reduce CPU usage by checking containers less frequently essential for Pi resource management
      - --housekeeping_interval=30s
      - --max_housekeeping_interval=60s
      - --event_storage_event_limit=default=0
      - --event_storage_age_limit=default=0
      # Prevents cAdvisor from trying to monitor the whole host, focusing only on containers
      - --docker_only=true
      # Enable filtering metrics in Grafana by the labels set in compose file
      - --store_container_labels=true
      # Added for Pi 5 / cgroup v2 handling:
      - --enable_load_reader=false
      - --allow_dynamic_housekeeping=true
      # Use this only if logs show "container not found in any cgroup"
      # - --cgroup_hierarchy_root=/sys/fs/cgroup # Forces cAdvisor to the correct Cgroup V2 root on Pi OS
    logging:
      driver: "json-file"
      options:
        max-size: "5m"

networks:
  monitoring:
    name: monitoring
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: br-monitoring
    ipam:
      config:
        - subnet: 172.20.0.0/16
          gateway: 172.20.0.1
